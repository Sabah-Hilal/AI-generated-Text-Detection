{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":352326,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":293953,"modelId":314580},{"sourceId":352340,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":293963,"modelId":314590}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:45:05.153106Z","iopub.execute_input":"2025-05-06T16:45:05.153422Z","iopub.status.idle":"2025-05-06T16:45:05.461672Z","shell.execute_reply.started":"2025-05-06T16:45:05.153401Z","shell.execute_reply":"2025-05-06T16:45:05.460964Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install gradio --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T16:45:05.463213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pymupdf --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install gradio PyMuPDF transformers torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\nimport torch\nimport torch.nn.functional as F\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nimport fitz  # PyMuPDF\n\n# Load both models\narabic_model_path = \"/kaggle/input/ara_model/flax/default/1\"\nenglish_model_path = \"/kaggle/input/eng_model/flax/default/1\"\n\narabic_tokenizer = DistilBertTokenizer.from_pretrained(arabic_model_path)\narabic_model = DistilBertForSequenceClassification.from_pretrained(arabic_model_path)\narabic_model.eval()\n\nenglish_tokenizer = DistilBertTokenizer.from_pretrained(english_model_path)\nenglish_model = DistilBertForSequenceClassification.from_pretrained(english_model_path)\nenglish_model.eval()\n\n# Extract text from PDF\ndef extract_text_from_pdf(file):\n    doc = fitz.open(stream=file.read(), filetype=\"pdf\")\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    return text\n\n# Prediction function\ndef predict(language, input_type, text, file):\n    if input_type == \"PDF\" and file is not None:\n        text = extract_text_from_pdf(file)\n    elif input_type == \"Text\":\n        text = text\n    else:\n        return \"Please provide a valid input.\"\n\n    if language == \"Arabic\":\n        tokenizer = arabic_tokenizer\n        model = arabic_model\n    else:\n        tokenizer = english_tokenizer\n        model = english_model\n\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        probs = F.softmax(outputs.logits, dim=1)\n        confidence, prediction = torch.max(probs, dim=1)\n        label = \"Human\" if prediction.item() == 0 else \"AI\"\n        result = f\"Prediction: {label} ({confidence.item()*100:.2f}%)\"\n        return result\n\n# Feedback collection\ndef collect_feedback(feedback):\n    return f\"Thanks for your feedback: {feedback}\"\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# AI vs Human Text Detector\")\n\n    language = gr.Radio([\"Arabic\", \"English\"], label=\"Select Language\", value=\"English\")\n    input_type = gr.Radio([\"Text\", \"PDF\"], label=\"Choose Input Type\", value=\"Text\")\n    \n    text_input = gr.Textbox(label=\"Enter your text\", visible=True)\n    pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"], visible=False)\n    \n    result_output = gr.Textbox(label=\"Prediction Result\", interactive=False)\n    feedback_button = gr.Button(\"Provide Feedback\", visible=False)\n    feedback_box = gr.Textbox(label=\"Your Feedback\", visible=False)\n    feedback_result = gr.Textbox(label=\"\", visible=False)\n\n    submit = gr.Button(\"Submit\")\n\n    def toggle_inputs(input_choice):\n        return (\n            gr.update(visible=input_choice == \"Text\"),\n            gr.update(visible=input_choice == \"PDF\")\n        )\n    \n    input_type.change(fn=toggle_inputs, inputs=input_type, outputs=[text_input, pdf_input])\n\n    def on_submit(language, input_type, text_input, pdf_input):\n        result = predict(language, input_type, text_input, pdf_input)\n        return result, gr.update(visible=True), gr.update(visible=True)\n\n    submit.click(fn=on_submit, inputs=[language, input_type, text_input, pdf_input],\n                 outputs=[result_output, feedback_button, feedback_box])\n\n    feedback_button.click(lambda: gr.update(visible=True), None, feedback_box)\n    feedback_box.submit(fn=collect_feedback, inputs=feedback_box, outputs=feedback_result)\n\ndemo.launch()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}