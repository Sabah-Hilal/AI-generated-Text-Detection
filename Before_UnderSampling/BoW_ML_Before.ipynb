{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rJsd-lN8o-5"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyO9yoLq9xj2",
        "outputId": "3f9564cb-f4ae-49f9-d027-6caef36bc86a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "def fast_clean(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|[^a-zA-Z\\s]\", \"\", text)\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "    return \" \".join([t for t in tokens if t not in stop_words])"
      ],
      "metadata": {
        "id": "ntJPdBLh9yx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0y4DM7e-VPa",
        "outputId": "d961893d-414c-4d5d-fd22-87189512eacc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"artem9k/ai-text-detection-pile\")[\"train\"]\n",
        "seen, texts, labels = set(), [], []\n",
        "for sample in dataset:\n",
        "    if sample[\"text\"] and sample[\"text\"].strip() and sample[\"text\"] not in seen:\n",
        "        seen.add(sample[\"text\"])\n",
        "        texts.append(sample[\"text\"])\n",
        "        labels.append(1 if sample[\"source\"] == \"ai\" else 0)\n",
        "\n",
        "df = pl.DataFrame({\"text\": texts, \"label\": labels})\n",
        "df = df.with_columns(pl.Series(\"cleaned_text\", [fast_clean(t) for t in df[\"text\"]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKGIH1Lo-UTB",
        "outputId": "0774020d-9c78-495d-e39a-9cebca94002a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df[\"cleaned_text\"].to_list(), df[\"label\"].to_list(),\n",
        "    test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "k649foYk-dS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "bow_vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
        "\n",
        "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
        "X_val_bow = bow_vectorizer.transform(X_val)"
      ],
      "metadata": {
        "id": "wL8aBl0sARtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "BqWKWHg-Cj7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model, name):\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model.fit(X_train_bow, y_train)\n",
        "    y_pred = model.predict(X_val_bow)\n",
        "    print(f\"\\n{name} Classification Report:\")\n",
        "    print(classification_report(y_val, y_pred))"
      ],
      "metadata": {
        "id": "QEBKEU0zCnBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(LogisticRegression(max_iter=1000), \"Logistic Regression\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLIF2jD7CrQA",
        "outputId": "9e1fd42c-56ba-42f3-bdb3-747679c70e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Logistic Regression...\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.95      0.93    204218\n",
            "           1       0.84      0.78      0.81     72951\n",
            "\n",
            "    accuracy                           0.90    277169\n",
            "   macro avg       0.88      0.86      0.87    277169\n",
            "weighted avg       0.90      0.90      0.90    277169\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_jobs=-1), \"XGBoost\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNO1Fc9uDMiF",
        "outputId": "d4ec76b9-8374-4a1b-aca4-d02b615ba8cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [21:27:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XGBoost Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.94      0.91    204218\n",
            "           1       0.78      0.65      0.71     72951\n",
            "\n",
            "    accuracy                           0.86    277169\n",
            "   macro avg       0.83      0.79      0.81    277169\n",
            "weighted avg       0.86      0.86      0.86    277169\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(RandomForestClassifier(n_jobs=-1), \"Random Forest\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt94FVLnDZ9z",
        "outputId": "72c54d21-d195-4917-cf17-afa6efec48e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Random Forest...\n",
            "\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.94      0.91    204218\n",
            "           1       0.80      0.67      0.73     72951\n",
            "\n",
            "    accuracy                           0.87    277169\n",
            "   macro avg       0.84      0.80      0.82    277169\n",
            "weighted avg       0.87      0.87      0.87    277169\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install scikit-learn-intelex\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ibi-JM0awxNf",
        "outputId": "5f5c6709-5dcb-4ede-8edf-d8ebdcae33b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn-intelex in /usr/local/lib/python3.11/dist-packages (2025.4.0)\n",
            "Requirement already satisfied: daal==2025.4.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn-intelex) (2025.4.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.11/dist-packages (from scikit-learn-intelex) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from scikit-learn-intelex) (1.6.1)\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from daal==2025.4.0->scikit-learn-intelex) (2022.1.0)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->daal==2025.4.0->scikit-learn-intelex) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->scikit-learn-intelex) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tanEljTkxvJV",
        "outputId": "a9acfcad-6a4c-41b2-c306-948506b02e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Intel(R) Extension for Scikit-learn* enabled (https://github.com/uxlfoundation/scikit-learn-intelex)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "normalizer = Normalizer(norm='l2')\n",
        "X_train_bow_norm = normalizer.fit_transform(X_train_bow)\n",
        "X_val_bow_norm = normalizer.transform(X_val_bow)"
      ],
      "metadata": {
        "id": "iFeH2JCAx7XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svm = SVC(kernel='rbf', C=1.0, random_state=42)\n",
        "svm.fit(X_train_bow_norm, y_train)\n",
        "\n",
        "y_pred = svm.predict(X_val_bow_norm)\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suCxqqDD1sSK",
        "outputId": "c0d6b04f-0257-41df-d032-374cc8767a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95    204218\n",
            "           1       0.87      0.87      0.87     72951\n",
            "\n",
            "    accuracy                           0.93    277169\n",
            "   macro avg       0.91      0.91      0.91    277169\n",
            "weighted avg       0.93      0.93      0.93    277169\n",
            "\n"
          ]
        }
      ]
    }
  ]
}