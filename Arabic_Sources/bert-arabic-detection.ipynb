{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11513155,"sourceType":"datasetVersion","datasetId":7219716}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW  \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom transformers import AutoTokenizer, DistilBertForSequenceClassification, get_scheduler\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T10:08:26.729958Z","iopub.execute_input":"2025-04-23T10:08:26.730225Z","iopub.status.idle":"2025-04-23T10:08:55.818564Z","shell.execute_reply.started":"2025-04-23T10:08:26.730203Z","shell.execute_reply":"2025-04-23T10:08:55.817684Z"}},"outputs":[{"name":"stderr","text":"2025-04-23 10:08:42.590710: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745402922.919039      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745402923.007243      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T10:09:10.506900Z","iopub.execute_input":"2025-04-23T10:09:10.507452Z","iopub.status.idle":"2025-04-23T10:09:10.510752Z","shell.execute_reply.started":"2025-04-23T10:09:10.507427Z","shell.execute_reply":"2025-04-23T10:09:10.510016Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/cleaned-arabic-text-svc-dataset-csv/cleaned_arabic_text_svc_dataset.csv\")\n\n# 2. Make sure 'label' column is already binary (0 = Human, 1 = AI)\nprint(df['label'].value_counts())  # Optional: view label distribution\n\n# 3. Split dataset\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    df['text'].tolist(), df['label'].tolist(), test_size=0.2, stratify=df['label'], random_state=42\n)\n\n# 4. Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"asafaya/bert-base-arabic\", num_labels=2)\n\n# 5. Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=512):\n        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# 6. Create DataLoaders\ntrain_dataset = TextDataset(train_texts, train_labels, tokenizer)\nval_dataset = TextDataset(val_texts, val_labels, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8)\n\n# 7. Training setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\nnum_training_steps = len(train_loader) * 3  # 3 epochs\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# 8. Train the model\nmodel.train()\nfor epoch in range(3):\n    print(f\"Epoch {epoch+1}\")\n    for batch in tqdm(train_loader):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n# 9. Save model & tokenizer\nmodel.save_pretrained(\"./arabic_model_v2\")\ntokenizer.save_pretrained(\"./arabic_model_v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:23:20.836574Z","iopub.execute_input":"2025-04-23T11:23:20.837150Z","iopub.status.idle":"2025-04-23T11:44:43.632964Z","shell.execute_reply.started":"2025-04-23T11:23:20.837130Z","shell.execute_reply":"2025-04-23T11:44:43.632324Z"}},"outputs":[{"name":"stdout","text":"label\n0    5000\n1    5000\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [07:06<00:00,  2.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [07:06<00:00,  2.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [07:06<00:00,  2.35it/s]\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"('./arabic_model_v2/tokenizer_config.json',\n './arabic_model_v2/special_tokens_map.json',\n './arabic_model_v2/vocab.txt',\n './arabic_model_v2/added_tokens.json',\n './arabic_model_v2/tokenizer.json')"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Switch to eval mode\nmodel.eval()\n\nall_preds = []\nall_labels = []\n\n# Disable gradient calculation for evaluation\nwith torch.no_grad():\n    for batch in val_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=1)\n\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Print classification report\nreport = classification_report(all_labels, all_preds, target_names=[\"Human\", \"AI\"])\nprint(report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:44:49.098554Z","iopub.execute_input":"2025-04-23T11:44:49.098821Z","iopub.status.idle":"2025-04-23T11:45:18.934782Z","shell.execute_reply.started":"2025-04-23T11:44:49.098802Z","shell.execute_reply":"2025-04-23T11:45:18.933934Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       Human       1.00      1.00      1.00      1000\n          AI       1.00      1.00      1.00      1000\n\n    accuracy                           1.00      2000\n   macro avg       1.00      1.00      1.00      2000\nweighted avg       1.00      1.00      1.00      2000\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive(\"arabic_model_v2\", 'zip', \"./arabic_model_v2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:46:07.726476Z","iopub.execute_input":"2025-04-23T11:46:07.726973Z","iopub.status.idle":"2025-04-23T11:46:30.349891Z","shell.execute_reply.started":"2025-04-23T11:46:07.726951Z","shell.execute_reply":"2025-04-23T11:46:30.349214Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/arabic_model_v2.zip'"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch.nn.functional as F\n\n# 1. Load saved model and tokenizer\nmodel_path = \"./arabic_model_v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\nmodel.eval()\n\n# 2. Move model to correct device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# 3. Function to predict single sample\ndef predict_text(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n    inputs = {key: val.to(device) for key, val in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        probs = F.softmax(logits, dim=1)\n        prediction = torch.argmax(probs, dim=1).item()\n        confidence = probs[0][prediction].item()\n\n    label_map = {0: \"Human\", 1: \"AI\"}\n    return label_map[prediction], confidence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:48:55.616485Z","iopub.execute_input":"2025-04-23T11:48:55.616718Z","iopub.status.idle":"2025-04-23T11:48:55.942430Z","shell.execute_reply.started":"2025-04-23T11:48:55.616701Z","shell.execute_reply":"2025-04-23T11:48:55.941745Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"text_sample = \"شعر سامر بالفضول، فتوجه إلى المدرسة القديمة، ووجد شجرة تين ضخمة. بدأ بالحفر تحتها، وهناك وجد صندوقًا صغيرًا يحتوي على صور قديمة ورسائل حب بين زوجين عاشا في ذلك الحي منذ أكثر من خمسين سنة. كانت القصة التي كُتبت في تلك الرسائل أجمل من أي كنز مادي.\"\nlabel, confidence = predict_text(text_sample)\nprint(f\"Prediction: {label}, Confidence: {confidence:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:49:30.473328Z","iopub.execute_input":"2025-04-23T11:49:30.473878Z","iopub.status.idle":"2025-04-23T11:49:30.520151Z","shell.execute_reply.started":"2025-04-23T11:49:30.473859Z","shell.execute_reply":"2025-04-23T11:49:30.519544Z"}},"outputs":[{"name":"stdout","text":"Prediction: AI, Confidence: 100.00%\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"text_sample = \"لذكاء الاصطناعي (AI) هو مجال من مجالات علوم الكمبيوتر يهدف إلى إنشاء أنظمة قادرة على محاكاة الذكاء البشري. يشمل الذكاء الاصطناعي تطوير الخوارزميات والنماذج التي تمكّن الآلات من أداء مهام تتطلب عادة ذكاء بشري مثل التعلم، والتفكير، واتخاذ القرارات، والتفاعل مع البيئة.\"\nlabel, confidence = predict_text(text_sample)\nprint(f\"Prediction: {label}, Confidence: {confidence:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:51:47.775906Z","iopub.execute_input":"2025-04-23T11:51:47.776515Z","iopub.status.idle":"2025-04-23T11:51:47.792928Z","shell.execute_reply.started":"2025-04-23T11:51:47.776493Z","shell.execute_reply":"2025-04-23T11:51:47.792338Z"}},"outputs":[{"name":"stdout","text":"Prediction: AI, Confidence: 98.07%\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"text_sample = \"فـــالكمبیوتر یمكـــن أن یطـــور الفكـــرة الواحـــدة بـــآلاف الأفكـــار، إلـــى جانـــب قدرتـــه علـــى التخـزین وتقـدیم البـدائل وعمـل صـیاغات لانهائیـة یمكـن الاسـتفادة منهـا فـي بنـاء العمـل الفنـي الواحـد بـل والتنـوع اللانهـائي لـه مـن حیـث اسـتخدام الحـذف والإضـافة ، وتعتبـر الألـوان وكـذا الـدرجات اللونیـة والخطـوط وملامـس الأشـكال والأرضـیات والمونتـاج بـین عناصـر العمـل الفنـي ووضعها في أطر بصریة متعددة بالإضـافة إلـى الدقـة الـشدیدة فـي صـیاغة الأشـكال بـل الأكثـر من ذلك یمكن استخدام الذاكرة في استرجاع أعمال سبق تخزینهـا واختیـار بعـض الأشـكال منهـا فـــى إنـــشاء عمـــل جدیـــد بالإضـــافة لإمكانیـــة طبـــع كـــل هـــذه التكونیـــات والمقارنـــة فیمـــا بینهـــا والاحتفاظ بها مسجلة لإعادة طباعتها عند الحاجة\"\nlabel, confidence = predict_text(text_sample)\nprint(f\"Prediction: {label}, Confidence: {confidence:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:55:20.435385Z","iopub.execute_input":"2025-04-23T11:55:20.435687Z","iopub.status.idle":"2025-04-23T11:55:20.466504Z","shell.execute_reply.started":"2025-04-23T11:55:20.435668Z","shell.execute_reply":"2025-04-23T11:55:20.465872Z"}},"outputs":[{"name":"stdout","text":"Prediction: Human, Confidence: 100.00%\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"text_sample =  \"فـــالكمبیوتر یمكـــن أن یطـــور الفكـــرة الواحـــدة بـــآلاف الأفكـــار، إلـــى جانـــب قدرتـــه علـــى التخـزین وتقـدیم البـدائل وعمـل صـیاغات لانهائیـة یمكـن الاسـتفادة منهـا فـي بنـاء العمـل الفنـي الواحـد بـل والتنـوع اللانهـائي لـه مـن حیـث اسـتخدام الحـذف والإضـافة ، وتعتبـر الألـوان وكـذا الـدرجات اللونیـة والخطـوط وملامـس الأشـكال والأرضـیات والمونتـاج بـین عناصـر العمـل الفنـي ووضعها في أطر بصریة متعددة بالإضـافة إلـى الدقـة الـشدیدة فـي صـیاغة الأشـكال بـل الأكثـر من ذلك یمكن استخدام الذاكرة في استرجاع أعمال سبق تخزینهـا واختیـار بعـض الأشـكال منهـا فـــى إنـــشاء عمـــل جدیـــد بالإضـــافة لإمكانیـــة طبـــع كـــل هـــذه التكونیـــات والمقارنـــة فیمـــا بینهـــا والاحتفاظ بها مسجلة لإعادة طباعتها عند الحاجة\"\nlabel, confidence = predict_text(text_sample)\nprint(f\"Prediction: {label}, Confidence: {confidence:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:57:32.698191Z","iopub.execute_input":"2025-04-23T11:57:32.698898Z","iopub.status.idle":"2025-04-23T11:57:32.729445Z","shell.execute_reply.started":"2025-04-23T11:57:32.698871Z","shell.execute_reply":"2025-04-23T11:57:32.728849Z"}},"outputs":[{"name":"stdout","text":"Prediction: Human, Confidence: 100.00%\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"text_sample = \"يهدف الذكاء الاصطناعي إلى جعل الآلات أكثر قدرة على التفكير والتعلم مثل البشر، ويستخدم في العديد من التطبيقات مثل السيارات الذاتية القيادة، المساعدات الرقمية، الترجمة التلقائية، وأدوات كشف النصوص المولدة بالذكاء الاصطناعي.\"\nlabel, confidence = predict_text(text_sample)\nprint(f\"Prediction: {label}, Confidence: {confidence:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:58:23.476348Z","iopub.execute_input":"2025-04-23T11:58:23.476891Z","iopub.status.idle":"2025-04-23T11:58:23.491925Z","shell.execute_reply.started":"2025-04-23T11:58:23.476866Z","shell.execute_reply":"2025-04-23T11:58:23.491311Z"}},"outputs":[{"name":"stdout","text":"Prediction: AI, Confidence: 99.92%\n","output_type":"stream"}],"execution_count":28}]}